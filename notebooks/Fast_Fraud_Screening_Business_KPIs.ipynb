{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be8d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import networkx as nx\n",
    "from sklearn.metrics import classification_report, accuracy_score, log_loss,average_precision_score,confusion_matrix,ConfusionMatrixDisplay\n",
    "import pickle\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "#used for calling models from other notebooks\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a075c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features=pd.read_csv('total_features', sep=',', header=0) \n",
    "total_data = pd.read_csv('fraud_payment_data', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a31be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data=total_data[total_data.USD_amount>0]\n",
    "total_data=total_data.reset_index(drop=True)\n",
    "total_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c78c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding USD to final features because they were not included in the modeling features \n",
    "final_features[\"USD_amount\"] = total_data[\"USD_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dbdbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train, test, split \n",
    "train_features = final_features[0:1000000]\n",
    "val_features = final_features[1000000:1250000]\n",
    "test_features = final_features[1250000:-1]\n",
    "\n",
    "y_train = total_data['Label'][0:1000000]\n",
    "y_val   = total_data['Label'][1000000:1250000]\n",
    "y_test  = total_data['Label'][1250000:-1]\n",
    "\n",
    "#Fit scaler on TRAIN ONLY - this learns mean and std from training set \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_features)\n",
    "\n",
    "# Transform val/test using the SAME scaler\n",
    "X_val  = scaler.transform(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4625ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model from Fast_Fraud_Screening_Model.ipynb\n",
    "with open('xgboost_fraud_model.pkl', 'rb') as f:\n",
    "    model_package = pickle.load(f)\n",
    "\n",
    "xgb_model = model_package['model']\n",
    "feature_names = model_package['feature_names']\n",
    "\n",
    "#Use scale test data for predictions\n",
    "df_test = pd.DataFrame(X_test, columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32853937",
   "metadata": {},
   "source": [
    "                                               Business KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e1250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SCALED test data\n",
    "df_test = pd.DataFrame(X_test, columns=feature_names)\n",
    "\n",
    "#Add labels\n",
    "df_test[\"is_fraud\"] = y_test.values\n",
    "\n",
    "# dd USD_amount from original test_features\n",
    "df_test[\"USD_amount\"] = test_features[\"USD_amount\"].values\n",
    "\n",
    "#Add model scores\n",
    "df_test[\"model_score\"] = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7313b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore thresholds to find optimal recall\n",
    "total_fraud_cases = df_test[\"is_fraud\"].sum()\n",
    "\n",
    "print(\"Evaluating threshold options:\")\n",
    "for thresh in [0.12, 0.14, 0.15, 0.16, 0.18, .20, .30, .40, .50]:\n",
    "    df_test[\"model_flag\"] = (df_test[\"model_score\"] >= thresh).astype(int)\n",
    "    caught = ((df_test[\"is_fraud\"] == 1) & (df_test[\"model_flag\"] == 1)).sum()\n",
    "    recall = caught / total_fraud_cases \n",
    "    print(f\"Threshold {thresh}: Recall = {recall:.2%}, Caught = {caught}\")\n",
    "\n",
    "\n",
    "#Threshold that achieves target recall\n",
    "threshold = 0.18\n",
    "df_test[\"model_flag\"] = (df_test[\"model_score\"] >= threshold).astype(int)\n",
    "\n",
    "#Recalculate recall for selected threshold\n",
    "caught = ((df_test[\"is_fraud\"] == 1) & (df_test[\"model_flag\"] == 1)).sum()\n",
    "recall = caught / total_fraud_cases\n",
    "\n",
    "print(f\"\\nSelected threshold: {threshold} (Recall: {recall:.2%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7ea7ae",
   "metadata": {},
   "source": [
    "                                               Synthetic Loss, Missed Risk and Loss Avoided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf32129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Calculate fraud dollar amounts\n",
    "#Total loss - Synthetic loss or fraud Loss Avoided score\n",
    "#missed fraud risk score\n",
    "#loss avoided accumulates only when we catch fraud\n",
    "\n",
    "df_test[\"synthetic_loss\"] = df_test[\"USD_amount\"] * df_test[\"is_fraud\"]\n",
    "df_test[\"missed_risk\"] = df_test[\"synthetic_loss\"] * (1- df_test[\"model_flag\"])\n",
    "df_test[\"loss_avoided\"] = df_test[\"synthetic_loss\"] * df_test[\"model_flag\"]\n",
    "\n",
    "#Aggregate Metrics\n",
    "total_loss = df_test[\"synthetic_loss\"].sum()\n",
    "missed_risk = df_test[\"missed_risk\"].sum()\n",
    "loss_avoided = df_test[\"loss_avoided\"].sum()\n",
    "\n",
    "#Caluclate percentages\n",
    "missed_risk_pct = missed_risk / total_loss\n",
    "loss_avoided_pct = loss_avoided / total_loss\n",
    "\n",
    "\n",
    "print(f\"Total Loss: ${total_loss:,.2f}\")\n",
    "print(f\"Loss Avoided: ${loss_avoided:,.2f} ({loss_avoided_pct:.1%})\")\n",
    "print(f\"Missed Risk: ${missed_risk:,.2f} ({missed_risk_pct:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd02696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average fraud amount: caught vs missed (fraud cases only)\n",
    "caught_frauds = (df_test[\"is_fraud\"] == 1) & (df_test[\"model_flag\"] == 1)\n",
    "missed_frauds = (df_test[\"is_fraud\"] == 1) & (df_test[\"model_flag\"] == 0)\n",
    "\n",
    "caught_fraud_avg = df_test[caught_frauds][\"USD_amount\"].mean()\n",
    "missed_fraud_avg = df_test[missed_frauds][\"USD_amount\"].mean()\n",
    "\n",
    "print(\"\\n=== Fraud Case Characteristics ===\")\n",
    "print(f\"Avg $ per caught fraud: ${caught_fraud_avg:.2f}\")\n",
    "print(f\"Avg $ per missed fraud: ${missed_fraud_avg:.2f}\")\n",
    "print(f\"Value ratio (caught/missed): {caught_fraud_avg/missed_fraud_avg:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e146565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fraud detection performance\n",
    "total_fraud_cases = df_test[\"is_fraud\"].sum()\n",
    "caught_fraud_cases = ((df_test[\"is_fraud\"] == 1) & (df_test[\"model_flag\"] == 1)).sum()\n",
    "missed_fraud_cases = ((df_test[\"is_fraud\"] == 1) & (df_test[\"model_flag\"] == 0)).sum()\n",
    "\n",
    "#Additional metrics\n",
    "total_flagged = df_test[\"model_flag\"].sum()\n",
    "precision = caught_fraud_cases / total_flagged if total_flagged > 0 else 0\n",
    "\n",
    "print(\"\\n=== Model Performance Summary ===\")\n",
    "print(f\"Total cases flagged: {total_flagged:,}\")\n",
    "print(f\"Total fraud cases: {total_fraud_cases:,}\")\n",
    "print(f\"Caught fraud cases: {caught_fraud_cases:,}\")\n",
    "print(f\"Missed fraud cases: {missed_fraud_cases:,}\")\n",
    "print(f\"Recall: {recall:.2%}\")\n",
    "print(f\"Precision: {precision:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vizualizing model performace on Fraud Losses\n",
    "plt.bar([\"Loss avoided\", \"Missed risk\"], [loss_avoided, missed_risk], color=[\"#42A2B9\", \"crimson\"])\n",
    "plt.title(\"Model Performance on Fraud Losses\")\n",
    "plt.ylabel(\"USD (Millions)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143bd7ee",
   "metadata": {},
   "source": [
    "## Alert Cost Simulation â€“ The Cost of Analysts Reviewing Alerts\n",
    "\n",
    "Cases vary in complexity, but this estimate acknowledges a balanced mix of simple, moderate, and complex cases.\n",
    "\n",
    "- Our model flags fraud and leads to an initial triage: 5-15 min to check basic patterns.\n",
    "- Simple false positives are cleared quiclkly.\n",
    "- While complex cases are escalated to a stricter model or to an analyst, and may take several hours to days\n",
    "\n",
    "Realistically, the estimated hourly rate per analyst ranges from:\n",
    "$5 - $15 for FinTech firms\n",
    "$16 - $25 for mid-size banks\n",
    "$26 - $70 for larger banks\n",
    "\n",
    "**Assumptions:**  \n",
    "- Standard analyst workday: 8 hours \n",
    "- Average review time: 15 minutes per case\n",
    "\n",
    "**Cost Estimate:**  \n",
    "Since our data is synthetic from **JPMorgan Chase**, we use average cost from larger banks ($50).\n",
    "\n",
    "- Average analyst hourly rate: **$50/hour**\n",
    "- Average review time: ** 15  minutes** per case\n",
    "\n",
    "**Synthetic Average Review Cost:**\n",
    "\n",
    "$$\n",
    "\\$12.50 = \\$50/\\text{hour} \\times 15\\ \\text{minutes}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5466e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revised realistic assumptions\n",
    "hourly_rate = 50  # $/hour\n",
    "avg_review_time_minutes = 0.25  # 15 minutes per case (weighted average)\n",
    "\n",
    "# Calculate average review cost per case\n",
    "synthetic_avg_review_cost = hourly_rate * avg_review_time_minutes\n",
    "\n",
    "print(f\"Revised average review cost per case: ${synthetic_avg_review_cost:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de3ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate review costs for flagged cases\n",
    "df_test[\"review_cost\"] = df_test[\"model_flag\"] * synthetic_avg_review_cost\n",
    "\n",
    "#Total operational cost\n",
    "total_review_cost = df_test[\"review_cost\"].sum()\n",
    "\n",
    "print(f\"\\n=== Operational Costs ===\")\n",
    "print(f\"Cases flagged for review: {df_test['model_flag'].sum():,}\")\n",
    "print(f\"Cost per case review: ${synthetic_avg_review_cost:.2f}\")\n",
    "print(f\"Total review cost: ${total_review_cost:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21557fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#False Positive Rate: % of legitimate transactions incorrectly flagged\n",
    "total_legit = (df_test[\"is_fraud\"] == 0).sum()\n",
    "false_positives = ((df_test[\"is_fraud\"] == 0) & (df_test[\"model_flag\"] == 1)).sum()\n",
    "fpr = false_positives / total_legit\n",
    "\n",
    "#False Negative Rate: % of frauds missed (complement of recall)\n",
    "fnr = 1 - recall \n",
    "\n",
    "print(\"\\n=== Error Rates ===\")\n",
    "print(f\"False Positive Rate: {fpr:.2%} ({false_positives:,} false alarms)\")\n",
    "print(f\"False Negative Rate: {fnr:.2%} (missed {missed_fraud_cases:,} frauds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef6ab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Calculate metrics across threshold range\n",
    "thresholds = np.arange(0.10, 0.50, 0.02)\n",
    "\n",
    "results = []\n",
    "for thresh in thresholds:\n",
    "    flagged = (df_test[\"model_score\"] >= thresh).sum()\n",
    "    flagged_pct = flagged / len(df_test)\n",
    "    \n",
    "    caught = ((df_test[\"is_fraud\"] == 1) & (df_test[\"model_score\"] >= thresh)).sum()\n",
    "    recall = caught / df_test[\"is_fraud\"].sum()\n",
    "    \n",
    "    precision = caught / flagged if flagged > 0 else 0\n",
    "    \n",
    "    #Financial metrics\n",
    "    df_test[\"temp_flag\"] = (df_test[\"model_score\"] >= thresh).astype(int)\n",
    "    loss_avoided_thresh = (df_test[\"synthetic_loss\"] * df_test[\"temp_flag\"]).sum()\n",
    "    review_cost_thresh = flagged * 12.50  # $12.50 per case\n",
    "    net_benefit = loss_avoided_thresh - review_cost_thresh\n",
    "    \n",
    "    results.append({\n",
    "        'threshold': thresh,\n",
    "        'flagged_pct': flagged_pct * 100,\n",
    "        'recall': recall * 100,\n",
    "        'precision': precision * 100,\n",
    "        'loss_avoided': loss_avoided_thresh / 1e6,  #millions\n",
    "        'review_cost': review_cost_thresh / 1e6,  #millions\n",
    "        'net_benefit': net_benefit / 1e6  #millions\n",
    "    })\n",
    "\n",
    "#Create DataFrame for easy plotting\n",
    "import pandas as pd\n",
    "df_threshold = pd.DataFrame(results)\n",
    "\n",
    "#Create visualizations\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "#Plot 1: Recall vs % Flagged\n",
    "ax1.plot(df_threshold['threshold'], df_threshold['recall'], 'g-', linewidth=2, label='Recall')\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1_twin.plot(df_threshold['threshold'], df_threshold['flagged_pct'], 'orange', linewidth=2, label='% Cases Flagged')\n",
    "ax1.axvline(x=0.16, color='red', linestyle='--', alpha=0.7, label='Current (0.16)')\n",
    "ax1.set_xlabel('Threshold')\n",
    "ax1.set_ylabel('Recall (%)', color='g')\n",
    "ax1_twin.set_ylabel('% Cases Flagged', color='orange')\n",
    "ax1.set_title('Recall vs Operational Volume')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(loc='upper left')\n",
    "ax1_twin.legend(loc='upper right')\n",
    "\n",
    "#Plot 2: Precision vs Recall\n",
    "ax2.plot(df_threshold['recall'], df_threshold['precision'], 'b-', linewidth=2)\n",
    "current_idx = df_threshold[df_threshold['threshold'].round(2) == 0.16].index[0]\n",
    "ax2.scatter(df_threshold.loc[current_idx, 'recall'], \n",
    "            df_threshold.loc[current_idx, 'precision'], \n",
    "            color='red', s=200, marker='*', zorder=5, label='Current (0.16)')\n",
    "ax2.set_xlabel('Recall (%)')\n",
    "ax2.set_ylabel('Precision (%)')\n",
    "ax2.set_title('Precision-Recall Trade-off')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "#Plot 3: Financial Impact\n",
    "ax3.plot(df_threshold['threshold'], df_threshold['loss_avoided'], 'g-', linewidth=2, label='Loss Avoided')\n",
    "ax3.plot(df_threshold['threshold'], df_threshold['review_cost'], 'orange', linewidth=2, label='Review Cost')\n",
    "ax3.plot(df_threshold['threshold'], df_threshold['net_benefit'], 'b--', linewidth=2, label='Net Benefit')\n",
    "ax3.axvline(x=0.16, color='red', linestyle='--', alpha=0.7, label='Current (0.16)')\n",
    "ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax3.set_xlabel('Threshold')\n",
    "ax3.set_ylabel('$ (Millions)')\n",
    "ax3.set_title('Financial Impact by Threshold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend()\n",
    "\n",
    "#Calculate lift in the results loop, adding to loop\n",
    "results = []\n",
    "for thresh in thresholds:\n",
    "    flagged = (df_test[\"model_score\"] >= thresh).sum()\n",
    "    flagged_pct = flagged / len(df_test)\n",
    "    \n",
    "    caught = ((df_test[\"is_fraud\"] == 1) & (df_test[\"model_score\"] >= thresh)).sum()\n",
    "    recall = caught / df_test[\"is_fraud\"].sum()\n",
    "    \n",
    "    precision = caught / flagged if flagged > 0 else 0\n",
    "    \n",
    "    # Calculate lift (recall / PPR)\n",
    "    lift = recall / flagged_pct if flagged_pct > 0 else 0\n",
    "    \n",
    "    # Financial metrics\n",
    "    df_test[\"temp_flag\"] = (df_test[\"model_score\"] >= thresh).astype(int)\n",
    "    loss_avoided_thresh = (df_test[\"synthetic_loss\"] * df_test[\"temp_flag\"]).sum()\n",
    "    review_cost_thresh = flagged * 12.50\n",
    "    net_benefit = loss_avoided_thresh - review_cost_thresh\n",
    "    \n",
    "    results.append({\n",
    "        'threshold': thresh,\n",
    "        'flagged_pct': flagged_pct * 100,\n",
    "        'recall': recall * 100,\n",
    "        'precision': precision * 100,\n",
    "        'lift': lift, \n",
    "        'loss_avoided': loss_avoided_thresh / 1e6,\n",
    "        'review_cost': review_cost_thresh / 1e6,\n",
    "        'net_benefit': net_benefit / 1e6\n",
    "    })\n",
    "\n",
    "df_threshold = pd.DataFrame(results)\n",
    "\n",
    "#Plot 4: Summary Table \n",
    "key_thresholds = [0.14, 0.15, 0.16, 0.18, 0.20, 0.25, 0.30]\n",
    "table_data = []\n",
    "for thresh in key_thresholds:\n",
    "    row = df_threshold[df_threshold['threshold'].round(2) == thresh]\n",
    "    if not row.empty:\n",
    "        table_data.append([\n",
    "            f\"{thresh:.2f}\",\n",
    "            f\"{row['recall'].values[0]:.1f}%\",\n",
    "            f\"{row['precision'].values[0]:.1f}%\",\n",
    "            f\"{row['lift'].values[0]:.2f}x\",\n",
    "            f\"{row['flagged_pct'].values[0]:.1f}%\",\n",
    "            f\"${row['loss_avoided'].values[0]:.2f}M\",\n",
    "            f\"${row['net_benefit'].values[0]:.2f}M\"\n",
    "        ])\n",
    "\n",
    "ax4.axis('tight')\n",
    "ax4.axis('off')\n",
    "table = ax4.table(\n",
    "    cellText=table_data,\n",
    "    colLabels=['Threshold', 'Recall', 'Precision', 'Lift', '% Flagged', 'Loss Avoided', 'Net Benefit'],\n",
    "    cellLoc='center',\n",
    "    loc='center',\n",
    "    colWidths=[0.10, 0.10, 0.10, 0.10, 0.12, 0.15, 0.15] \n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(8) \n",
    "table.scale(1, 2)\n",
    "\n",
    "#Highlight current threshold row\n",
    "for i in range(len(table_data)):\n",
    "    if table_data[i][0] == '0.16':\n",
    "        for j in range(len(table_data[i])):\n",
    "            table[(i+1, j)].set_facecolor('#ffcccc')\n",
    "\n",
    "ax4.set_title('Threshold Comparison Table', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('threshold_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "#Print with lift added\n",
    "print(\"\\n=== Threshold Analysis Summary ===\")\n",
    "print(f\"\\nCurrent Threshold (0.16):\")\n",
    "current = df_threshold[df_threshold['threshold'].round(2) == 0.16].iloc[0]\n",
    "print(f\"  Recall: {current['recall']:.1f}%\")\n",
    "print(f\"  Precision: {current['precision']:.1f}%\")\n",
    "print(f\"  Lift: {current['lift']:.2f}x\")  \n",
    "print(f\"  % Flagged: {current['flagged_pct']:.1f}%\")\n",
    "print(f\"  Loss Avoided: ${current['loss_avoided']:.2f}M\")\n",
    "print(f\"  Review Cost: ${current['review_cost']:.2f}M\")\n",
    "print(f\"  Net Benefit: ${current['net_benefit']:.2f}M\")\n",
    "\n",
    "#Optimal threshold\n",
    "optimal_idx = df_threshold['net_benefit'].idxmax()\n",
    "optimal = df_threshold.loc[optimal_idx]\n",
    "print(f\"\\nOptimal Threshold (Max Net Benefit: {optimal['threshold']:.2f}):\")\n",
    "print(f\"  Recall: {optimal['recall']:.1f}%\")\n",
    "print(f\"  Precision: {optimal['precision']:.1f}%\")\n",
    "print(f\"  Lift: {optimal['lift']:.2f}x\") \n",
    "print(f\"  % Flagged: {optimal['flagged_pct']:.1f}%\")\n",
    "print(f\"  Loss Avoided: ${optimal['loss_avoided']:.2f}M\")\n",
    "print(f\"  Review Cost: ${optimal['review_cost']:.2f}M\")\n",
    "print(f\"  Net Benefit: ${optimal['net_benefit']:.2f}M\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_ds_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
